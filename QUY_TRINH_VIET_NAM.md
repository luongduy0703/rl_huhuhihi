# Há»‡ Thá»‘ng Robot Arm Deep RL - Quy TrÃ¬nh LÃ m Viá»‡c HoÃ n Chá»‰nh

## ğŸ”„ Tá»•ng Quan Quy TrÃ¬nh LÃ m Viá»‡c

Há»‡ thá»‘ng tuÃ¢n theo **chu ká»³ há»c táº­p liÃªn tá»¥c** nÃ y:

### **Giai Äoáº¡n 1: Thiáº¿t Láº­p MÃ´i TrÆ°á»ng & Khá»Ÿi Táº¡o**
```
1. Khá»Ÿi Táº¡o Pháº§n Cá»©ng
   â””â”€ Thiáº¿t láº­p bá»™ Ä‘iá»u khiá»ƒn PWM PCA9685 (50Hz)
   â””â”€ 4 servo motor káº¿t ná»‘i vá»›i kÃªnh 0-3
   â””â”€ Thiáº¿t láº­p giao tiáº¿p I2C (GPIO2/GPIO3)
   â””â”€ Äáº·t giá»›i háº¡n an toÃ n (0-180Â° má»—i khá»›p)

2. Táº¡o MÃ´i TrÆ°á»ng RL
   â””â”€ Vá»‹ trÃ­ má»¥c tiÃªu Ä‘Æ°á»£c táº¡o ngáº«u nhiÃªn [x,y,z]
   â””â”€ CÃ¡nh tay robot reset vá» vá»‹ trÃ­ trung tÃ­nh (90Â° táº¥t cáº£ khá»›p)
   â””â”€ Vector tráº¡ng thÃ¡i Ä‘Æ°á»£c khá»Ÿi táº¡o (10 pháº§n tá»­)
```

### **Giai Äoáº¡n 2: VÃ²ng Láº·p Huáº¥n Luyá»‡n ChÃ­nh** (Láº·p láº¡i cho má»—i episode)

#### **BÆ°á»›c 1: Quan SÃ¡t Tráº¡ng ThÃ¡i**
```python
# MÃ´i trÆ°á»ng cung cáº¥p tráº¡ng thÃ¡i hiá»‡n táº¡i (10 giÃ¡ trá»‹):
state = [
    joint_angles[4],      # Chuáº©n hÃ³a vá» [-1,1]
    target_position[3],   # Tá»a Ä‘á»™ 3D [x,y,z]
    end_effector_pos[3]   # Vá»‹ trÃ­ Ä‘áº§u cÃ¡nh tay hiá»‡n táº¡i
]
```

#### **BÆ°á»›c 2: Ra Quyáº¿t Äá»‹nh Cá»§a AI**
```python
# RL Agent (DDPG/DQN) xá»­ lÃ½ tráº¡ng thÃ¡i:
action = agent.act(state)  # Tráº£ vá» 4 giÃ¡ trá»‹ [-1,1] cho má»—i khá»›p
```

#### **BÆ°á»›c 3: Chuyá»ƒn Äá»•i HÃ nh Äá»™ng**
```python
# Chuyá»ƒn Ä‘á»•i hÃ nh Ä‘á»™ng RL thÃ nh gÃ³c servo:
for joint in range(4):
    angle_change = action[joint] * max_change  # Tá»· lá»‡ vá» Ä‘á»™
    new_angle = current_angle + angle_change
    new_angle = clamp(new_angle, 0, 180)      # Giá»›i háº¡n an toÃ n
```

#### **BÆ°á»›c 4: Thá»±c Thi Pháº§n Cá»©ng**
```python
# Chuyá»ƒn Ä‘á»•i gÃ³c thÃ nh tÃ­n hiá»‡u PWM:
for joint in range(4):
    pulse_width = 500 + (angle/180) * (2500-500)  # Î¼s
    duty_cycle = pulse_width * 65535 / 20000      # 16-bit
    pca.channels[joint].duty_cycle = duty_cycle   # Gá»­i tá»›i pháº§n cá»©ng
```

#### **BÆ°á»›c 5: Chuyá»ƒn Äá»™ng Váº­t LÃ½**
```
PCA9685 â†’ Táº¡o PWM 50Hz â†’ Servo xoay â†’ CÃ¡nh tay robot di chuyá»ƒn
```

#### **BÆ°á»›c 6: Pháº£n Há»“i & Pháº§n ThÆ°á»Ÿng**
```python
# TÃ­nh toÃ¡n tráº¡ng thÃ¡i má»›i:
new_end_position = forward_kinematics(new_joint_angles)
new_state = create_state_vector(...)

# TÃ­nh toÃ¡n pháº§n thÆ°á»Ÿng:
distance_improvement = old_distance - new_distance
target_bonus = 10.0 if distance < 0.05m else 0.0
movement_penalty = -0.01 * sum(joint_changes)
total_reward = distance_improvement + target_bonus + movement_penalty
```

#### **BÆ°á»›c 7: Cáº­p Nháº­t Há»c Táº­p**
```python
# LÆ°u trá»¯ kinh nghiá»‡m:
agent.memory.store(state, action, reward, new_state, done)

# Huáº¥n luyá»‡n máº¡ng neural (má»—i 32 kinh nghiá»‡m):
if len(memory) > batch_size:
    agent.replay()  # Huáº¥n luyá»‡n backpropagation
```

### **Giai Äoáº¡n 3: HoÃ n ThÃ nh Episode**
```
Episode káº¿t thÃºc khi:
- Äáº¡t má»¥c tiÃªu (khoáº£ng cÃ¡ch < 5cm) â†’ THÃ€NH CÃ”NG!
- Äáº¡t sá»‘ bÆ°á»›c tá»‘i Ä‘a (200 bÆ°á»›c) â†’ Tiáº¿p tá»¥c há»c
- Vi pháº¡m an toÃ n â†’ Reset vÃ  thá»­ láº¡i
```

---

## ğŸ¯ SÆ¡ Äá»“ Quy TrÃ¬nh Há»‡ Thá»‘ng HoÃ n Chá»‰nh

```
Báº®T Äáº¦U
  â†“
ğŸ”§ THIáº¾T Láº¬P PHáº¦N Cá»¨NG
  â”œâ”€ Khá»Ÿi táº¡o PCA9685 (I2C, 50Hz)
  â”œâ”€ Káº¿t ná»‘i 4 servo vá»›i kÃªnh 0-3
  â””â”€ Äáº·t giá»›i háº¡n an toÃ n (0-180Â°)
  â†“
ğŸ² Báº®T Äáº¦U EPISODE
  â”œâ”€ Táº¡o vá»‹ trÃ­ má»¥c tiÃªu ngáº«u nhiÃªn
  â”œâ”€ Reset robot vá» trung tÃ­nh (90Â° táº¥t cáº£ khá»›p)
  â””â”€ Táº¡o vector tráº¡ng thÃ¡i ban Ä‘áº§u (10 pháº§n tá»­)
  â†“
ğŸ”„ VÃ’NG Láº¶P ÄIá»€U KHIá»‚N CHÃNH (Láº·p ~200 láº§n má»—i episode)
  â†“
ğŸ“Š 1. QUAN SÃT TRáº NG THÃI
  â”œâ”€ Äá»c gÃ³c khá»›p hiá»‡n táº¡i
  â”œâ”€ TÃ­nh vá»‹ trÃ­ Ä‘áº§u cÃ¡nh tay (forward kinematics)
  â””â”€ Káº¿t há»£p thÃ nh vector tráº¡ng thÃ¡i: [khá»›p(4) + má»¥c tiÃªu(3) + vá»‹_trÃ­_Ä‘áº§u(3)]
  â†“
ğŸ§  2. QUYáº¾T Äá»ŠNH Cá»¦A AI
  â”œâ”€ ÄÆ°a tráº¡ng thÃ¡i vÃ o máº¡ng neural (DDPG/DQN)
  â”œâ”€ Máº¡ng xuáº¥t vector hÃ nh Ä‘á»™ng (4 giÃ¡ trá»‹, -1 Ä‘áº¿n +1)
  â””â”€ ThÃªm nhiá»…u khÃ¡m phÃ¡ cho viá»‡c há»c
  â†“
âš™ï¸ 3. CHUYá»‚N Äá»”I HÃ€NH Äá»˜NG
  â”œâ”€ Tá»· lá»‡ hÃ nh Ä‘á»™ng thÃ nh thay Ä‘á»•i gÃ³c (tá»‘i Ä‘a Â±15Â° má»—i bÆ°á»›c)
  â”œâ”€ Cá»™ng vÃ o gÃ³c khá»›p hiá»‡n táº¡i
  â””â”€ Giá»›i háº¡n trong pháº¡m vi an toÃ n (0-180Â°)
  â†“
ğŸ“¡ 4. Táº O TIN HIá»†U PWM
  â”œâ”€ Chuyá»ƒn gÃ³c thÃ nh Ä‘á»™ rá»™ng xung (500-2500Î¼s)
  â”œâ”€ Chuyá»ƒn thÃ nh duty cycle 16-bit
  â””â”€ Gá»­i lá»‡nh I2C tá»›i PCA9685
  â†“
ğŸ¦¾ 5. THá»°C THI Váº¬T LÃ
  â”œâ”€ PCA9685 táº¡o tÃ­n hiá»‡u PWM 50Hz
  â”œâ”€ Servo chuyá»ƒn Ä‘á»™ rá»™ng xung thÃ nh vá»‹ trÃ­
  â””â”€ CÃ¡nh tay robot di chuyá»ƒn Ä‘áº¿n cáº¥u hÃ¬nh má»›i
  â†“
ğŸ“ 6. ÄO LÆ¯á»œNG & PHáº¢N Há»’I
  â”œâ”€ TÃ­nh vá»‹ trÃ­ Ä‘áº§u cÃ¡nh tay má»›i
  â”œâ”€ Äo khoáº£ng cÃ¡ch tá»›i má»¥c tiÃªu
  â””â”€ XÃ¡c Ä‘á»‹nh cÃ³ Ä‘áº¡t má»¥c tiÃªu khÃ´ng
  â†“
ğŸ† 7. TÃNH TOÃN PHáº¦N THÆ¯á»NG
  â”œâ”€ Cáº£i thiá»‡n khoáº£ng cÃ¡ch: (khoáº£ng_cÃ¡ch_cÅ© - khoáº£ng_cÃ¡ch_má»›i)
  â”œâ”€ ThÆ°á»Ÿng má»¥c tiÃªu: +10 náº¿u khoáº£ng cÃ¡ch < 5cm
  â”œâ”€ Pháº¡t di chuyá»ƒn: -0.01 * tá»•ng_di_chuyá»ƒn
  â””â”€ Tá»•ng pháº§n thÆ°á»Ÿng = cáº£i_thiá»‡n + thÆ°á»Ÿng + pháº¡t
  â†“
ğŸ’¾ 8. LÆ¯U TRá»® KINH NGHIá»†M
  â”œâ”€ LÆ°u: (tráº¡ng_thÃ¡i, hÃ nh_Ä‘á»™ng, pháº§n_thÆ°á»Ÿng, tráº¡ng_thÃ¡i_tiáº¿p, hoÃ n_thÃ nh)
  â””â”€ ThÃªm vÃ o bá»™ nhá»› replay
  â†“
ğŸ“ 9. Cáº¬P NHáº¬T Há»ŒC Táº¬P
  â”œâ”€ Láº¥y máº«u batch ngáº«u nhiÃªn tá»« bá»™ nhá»› (32 kinh nghiá»‡m)
  â”œâ”€ Huáº¥n luyá»‡n máº¡ng actor (cáº£i thiá»‡n chÃ­nh sÃ¡ch)
  â”œâ”€ Huáº¥n luyá»‡n máº¡ng critic (Æ°á»›c lÆ°á»£ng giÃ¡ trá»‹)
  â””â”€ Cáº­p nháº­t target networks (á»•n Ä‘á»‹nh)
  â†“
â“ EPISODE HOÃ€N THÃ€NH?
  â”œâ”€ CÃ“: Äáº¡t má»¥c tiÃªu â†’ ğŸ¯ THÃ€NH CÃ”NG! â†’ Episode tiáº¿p theo
  â”œâ”€ CÃ“: Háº¿t bÆ°á»›c tá»‘i Ä‘a â†’ ğŸ”„ Tiáº¿p tá»¥c há»c â†’ Episode tiáº¿p theo  
  â””â”€ KHÃ”NG: Tiáº¿p tá»¥c vÃ²ng láº·p â†’ Quay láº¡i QUAN SÃT TRáº NG THÃI
  â†“
ğŸ“ˆ TIáº¾N TRÃŒNH HUáº¤N LUYá»†N
  â”œâ”€ Theo dÃµi tá»· lá»‡ thÃ nh cÃ´ng qua cÃ¡c episode
  â”œâ”€ GiÃ¡m sÃ¡t pháº§n thÆ°á»Ÿng trung bÃ¬nh
  â”œâ”€ LÆ°u model cÃ³ hiá»‡u suáº¥t tá»‘t nháº¥t
  â””â”€ Tiáº¿p tá»¥c cho Ä‘áº¿n há»™i tá»¥ (80%+ thÃ nh cÃ´ng)
  â†“
ğŸ¯ Sáº´N SÃ€NG TRIá»‚N KHAI!
```

---

## âš¡ Äáº·c TÃ­nh ChÃ­nh Cá»§a Quy TrÃ¬nh

### **Thá»i Gian & Hiá»‡u Suáº¥t**
- **VÃ²ng Láº·p Äiá»u Khiá»ƒn**: 20Hz (50ms má»—i chu ká»³)
- **Äá»™ DÃ i Episode**: ~10 giÃ¢y (tá»‘i Ä‘a 200 bÆ°á»›c)
- **Tá»‘c Äá»™ Há»c**: Cáº£i thiá»‡n tháº¥y Ä‘Æ°á»£c sau ~50 episode
- **Há»™i Tá»¥**: 200-500 episode Ä‘á»ƒ cÃ³ hiá»‡u suáº¥t tá»‘t

### **Tá»‘c Äá»™ Luá»“ng Dá»¯ Liá»‡u**
- **Cáº­p Nháº­t Tráº¡ng ThÃ¡i**: 20 cáº­p nháº­t/giÃ¢y
- **Lá»‡nh HÃ nh Äá»™ng**: 4 lá»‡nh servo má»—i chu ká»³
- **TÃ­n Hiá»‡u PWM**: LiÃªn tá»¥c 50Hz tá»›i má»—i servo
- **Cáº­p Nháº­t Há»c Táº­p**: Má»—i 32 kinh nghiá»‡m (~1.6 giÃ¢y)

### **Chá»‰ Sá»‘ ThÃ nh CÃ´ng**
- **Äá»™ ChÃ­nh XÃ¡c Má»¥c TiÃªu**: Äá»™ chÃ­nh xÃ¡c Â±5cm cáº§n thiáº¿t
- **Tiáº¿n TrÃ¬nh Há»c**: Tá»· lá»‡ thÃ nh cÃ´ng tÄƒng theo thá»i gian
- **Hiá»‡u Quáº£**: Ãt bÆ°á»›c hÆ¡n cáº§n thiáº¿t Ä‘á»ƒ Ä‘áº¡t má»¥c tiÃªu
- **Äá»™ Bá»n Vá»¯ng**: Hiá»‡u suáº¥t nháº¥t quÃ¡n qua cÃ¡c má»¥c tiÃªu khÃ¡c nhau

---

## ğŸ”§ Giáº£i ThÃ­ch Chi Tiáº¿t Vá» MÃ´i TrÆ°á»ng Reinforcement Learning

### **Biá»ƒu Diá»…n Tráº¡ng ThÃ¡i MÃ´i TrÆ°á»ng**
MÃ´i trÆ°á»ng cÃ¡nh tay robot cung cáº¥p thÃ´ng tin tráº¡ng thÃ¡i cho RL agent:

#### **Vector Tráº¡ng ThÃ¡i (10 pháº§n tá»­):**
1. **GÃ³c Khá»›p (4)**: Vá»‹ trÃ­ hiá»‡n táº¡i Ä‘Ã£ chuáº©n hÃ³a [-1, 1]
2. **Vá»‹ TrÃ­ Má»¥c TiÃªu (3)**: Tá»a Ä‘á»™ 3D cá»§a má»¥c tiÃªu [x, y, z]
3. **Vá»‹ TrÃ­ Äáº§u CÃ¡nh Tay (3)**: Vá»‹ trÃ­ Ä‘áº§u cÃ¡nh tay hiá»‡n táº¡i [x, y, z]

#### **Xá»­ LÃ½ Tráº¡ng ThÃ¡i:**
```python
def _get_observation(self):
    # Chuáº©n hÃ³a gÃ³c khá»›p vá» [-1, 1]
    normalized_angles = 2 * (current_angles - min_angle) / (max_angle - min_angle) - 1
    
    # Láº¥y vá»‹ trÃ­ Ä‘áº§u cÃ¡nh tay qua forward kinematics
    end_pos = self._forward_kinematics(self.current_joint_angles)
    
    # Káº¿t há»£p thÃ nh vector tráº¡ng thÃ¡i
    state = [normalized_angles, target_position, end_pos]
    return state
```

### **KhÃ´ng Gian HÃ nh Äá»™ng**
RL agent xuáº¥t cÃ¡c hÃ nh Ä‘á»™ng Ä‘iá»u khiá»ƒn chuyá»ƒn Ä‘á»™ng servo:

#### **HÃ nh Äá»™ng LiÃªn Tá»¥c (DDPG):**
- **Pháº¡m Vi**: [-1, 1] cho má»—i khá»›p
- **Chuyá»ƒn Äá»•i**: hÃ nh_Ä‘á»™ng â†’ gÃ³c qua tá»· lá»‡ tuyáº¿n tÃ­nh
- **VÃ­ Dá»¥**: action=0.5 â†’ 135Â° (3/4 pháº¡m vi tá»« 0-180Â°)

#### **HÃ nh Äá»™ng Rá»i Ráº¡c (DQN):**
- **Táº­p HÃ nh Äá»™ng**: {-10Â°, -5Â°, 0Â°, +5Â°, +10Â°} má»—i khá»›p
- **Tá»• Há»£p**: 5^4 = 625 hÃ nh Ä‘á»™ng cÃ³ thá»ƒ
- **Lá»±a Chá»n**: Agent chá»n chá»‰ sá»‘ hÃ nh Ä‘á»™ng Ä‘Æ¡n láº»

### **Thiáº¿t Káº¿ HÃ m Pháº§n ThÆ°á»Ÿng**
Há»‡ thá»‘ng pháº§n thÆ°á»Ÿng dáº¡y robot Ä‘áº¡t má»¥c tiÃªu hiá»‡u quáº£:

#### **CÃ¡c ThÃ nh Pháº§n:**
```python
def _calculate_reward(self):
    # 1. Pháº§n thÆ°á»Ÿng cáº£i thiá»‡n khoáº£ng cÃ¡ch
    current_distance = ||end_position - target_position||
    distance_reward = previous_distance - current_distance
    
    # 2. ThÆ°á»Ÿng Ä‘áº¡t má»¥c tiÃªu
    target_bonus = 10.0 if current_distance < 0.05 else 0.0
    
    # 3. Pháº¡t di chuyá»ƒn (chuyá»ƒn Ä‘á»™ng mÆ°á»£t)
    movement_penalty = -0.01 * sum(|angle_changes|)
    
    # 4. Tá»•ng pháº§n thÆ°á»Ÿng
    total_reward = distance_reward + target_bonus + movement_penalty
    return total_reward
```

#### **Pháº¡m Vi Pháº§n ThÆ°á»Ÿng:**
- **Cáº£i thiá»‡n khoáº£ng cÃ¡ch**: -âˆ Ä‘áº¿n +âˆ (thÆ°á»ng -0.5 Ä‘áº¿n +0.5)
- **ThÆ°á»Ÿng má»¥c tiÃªu**: 0 hoáº·c +10.0
- **Pháº¡t di chuyá»ƒn**: -0.01 Ä‘áº¿n -0.5
- **Tá»•ng pháº¡m vi**: -âˆ Ä‘áº¿n +10.0 (thÆ°á»ng -2.0 Ä‘áº¿n +10.0)

Quy trÃ¬nh nÃ y láº·p láº¡i liÃªn tá»¥c, vá»›i AI dáº§n há»c cÃ¡c chÃ­nh sÃ¡ch tá»‘t hÆ¡n Ä‘á»ƒ Ä‘iá»u khiá»ƒn cÃ¡nh tay robot Ä‘áº¡t báº¥t ká»³ vá»‹ trÃ­ má»¥c tiÃªu nÃ o trong khÃ´ng gian lÃ m viá»‡c cá»§a nÃ³!
